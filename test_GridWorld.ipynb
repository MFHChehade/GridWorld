{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 episodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes:  15%|█▍        | 1487/10000 [02:20<13:25, 10.57 episodes/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Train the Q-learning agent\u001b[39;00m\n\u001b[0;32m     19\u001b[0m n_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m  \u001b[38;5;66;03m# Adjust the number of episodes as needed\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m rewards_per_episode \u001b[38;5;241m=\u001b[39m \u001b[43mq_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Plot the moving average of rewards per episode\u001b[39;00m\n\u001b[0;32m     23\u001b[0m q_agent\u001b[38;5;241m.\u001b[39mplot_moving_average_rewards(rewards_per_episode)\n",
      "File \u001b[1;32mc:\\Users\\HP\\UT\\Term I\\Research\\Code\\Developed Code\\Classes\\ql.py:89\u001b[0m, in \u001b[0;36mQLearning.train\u001b[1;34m(self, n_episodes, max_steps, verbose)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[0;32m     88\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchoose_action(state)\n\u001b[1;32m---> 89\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(state, action, reward, next_state, done)\n\u001b[0;32m     91\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n",
      "File \u001b[1;32mc:\\Users\\HP\\UT\\Term I\\Research\\Code\\Developed Code\\Classes\\grid_world.py:164\u001b[0m, in \u001b[0;36mGridWorld.step\u001b[1;34m(self, input_action)\u001b[0m\n\u001b[0;32m    161\u001b[0m numbers \u001b[38;5;241m=\u001b[39m [input_action] \u001b[38;5;241m+\u001b[39m other_actions\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# Sample an action with the specified probabilities\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m sampled_action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(numbers, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransition_probabilities)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# Get the new state after the agent's movement based on the sampled action\u001b[39;00m\n\u001b[0;32m    167\u001b[0m new_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmove_agent(sampled_action)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ql import QLearning\n",
    "from grid_world import GridWorld \n",
    "import numpy as np\n",
    "\n",
    "n = 10  # Adjust the grid size as needed\n",
    "\n",
    "# Assuming GridWorld and QLearning classes are defined\n",
    "\n",
    "rewards1 = np.full((n, n), 0.1)\n",
    "rewards1[0, 9] = 10 # Terminal state\n",
    "\n",
    "# Create a GridWorld environment\n",
    "grid_env = GridWorld(height=10, terminal_states=[(9,0)], rewards=rewards1, initial_state=(0,0))  # You can adjust the dimensions as needed\n",
    "\n",
    "# Initialize Q-learning agent with the environment\n",
    "q_agent = QLearning(env=grid_env, alpha=0.1, gamma=0.9, epsilon=0.1, infinite_horizon=False)\n",
    "\n",
    "# Train the Q-learning agent\n",
    "n_episodes = 10000  # Adjust the number of episodes as needed\n",
    "rewards_per_episode = q_agent.train(n_episodes=n_episodes, max_steps=1000, verbose=True)\n",
    "\n",
    "# Plot the moving average of rewards per episode\n",
    "q_agent.plot_moving_average_rewards(rewards_per_episode)\n",
    "\n",
    "\n",
    "# Get policy, value function, and Q-values\n",
    "policy = q_agent.get_policy()\n",
    "value_function = q_agent.get_value_function()\n",
    "Q_values = q_agent.get_q_values()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
